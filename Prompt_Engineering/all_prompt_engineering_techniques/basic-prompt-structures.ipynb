{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Prompt Structures Tutorial\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial focuses on two fundamental types of prompt structures:\n",
        "1. Single-turn prompts\n",
        "2. Multi-turn prompts (conversations)\n",
        "\n",
        "We'll use OpenAI's GPT model and LangChain to demonstrate these concepts.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Understanding different prompt structures is crucial for effective communication with AI models. Single-turn prompts are useful for quick, straightforward queries, while multi-turn prompts enable more complex, context-aware interactions. Mastering these structures allows for more versatile and effective use of AI in various applications.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Single-turn Prompts**: One-shot interactions with the language model.\n",
        "2. **Multi-turn Prompts**: Series of interactions that maintain context.\n",
        "3. **Prompt Templates**: Reusable structures for consistent prompting.\n",
        "4. **Conversation Chains**: Maintaining context across multiple interactions.\n",
        "\n",
        "## Method Details\n",
        "\n",
        "We'll use a combination of OpenAI's API and LangChain library to demonstrate these prompt structures. The tutorial will include practical examples and comparisons of different prompt types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Ollama on Windows (port 11434).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # OpenAI API key\n",
        "# Initialize the language model\n",
        "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "from select_llm import get_llm, get_supported_ollama_models\n",
        "ollama_model = \"llama3.2:1b\"  # or None for OpenAI\n",
        "# ollama_model = \"tinyllama:latest\"  # or None for OpenAI\n",
        "#ollama_model = \"deepseek-r1:7b\"\n",
        "llm = get_llm(ollama_model) #,force_port=12434)\n",
        "if llm is None:\n",
        "    models = get_supported_ollama_models()\n",
        "    print(\"Available Ollama models:\", models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Single-turn Prompts\n",
        "\n",
        "Single-turn prompts are one-shot interactions with the language model. They consist of a single input (prompt) and generate a single output (response)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The three primary colors are:\n",
            "1. Red\n",
            "2. Blue\n",
            "3. Yellow\n"
          ]
        }
      ],
      "source": [
        "single_turn_prompt = \"What are the three primary colors?\"\n",
        "print(llm.invoke(single_turn_prompt).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's use a PromptTemplate to create a more structured single-turn prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "structured_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Provide a brief explanation of {topic} and list its three main components.\"\n",
        ")\n",
        "\n",
        "chain = structured_prompt | llm\n",
        "print(chain.invoke({\"topic\": \"color theory\"}).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Multi-turn Prompts (Conversations)\n",
        "\n",
        "Multi-turn prompts involve a series of interactions with the language model, allowing for more complex and context-aware conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "print(conversation.predict(input=\"Hi, I'm learning about space. Can you tell me about planets?\"))\n",
        "print(conversation.predict(input=\"What's the largest planet in our solar system?\"))\n",
        "print(conversation.predict(input=\"How does its size compare to Earth?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modern simple equivalent (no deprecated ConversationChain)\n",
        "# This recreates the classic \"Current conversation:\\n... Human: ...\\nAI:\" prompt style\n",
        "# (if conversiona prediction is selected and manually manages the running history.\n",
        "\n",
        "red = \"\\033[91m\"\n",
        "blue = \"\\033[94m\"\n",
        "green = \"\\033[92m\"\n",
        "reset = \"\\033[0m\"\n",
        "\n",
        "SYSTEM_PREAMBLE = (\n",
        "    \"The following is a friendly conversation between a human and an AI. \"\n",
        "    \"The AI is talkative and provides lots of specific details from its context. \"\n",
        "    \"If the AI does not know the answer to a question, it truthfully says it does not know.\"\n",
        ")\n",
        "\n",
        "_history = \"\"\n",
        "\n",
        "def conversation(user_prompt: str, predict_conversation: bool = False, verbose: bool = True) -> str:\n",
        "    global _history\n",
        "\n",
        "    formatted_prompt = (\n",
        "        f\"{SYSTEM_PREAMBLE}\\n\\n\"\n",
        "        f\"Current conversation:\\n\"\n",
        "        f\"{_history}\"\n",
        "        f\"Human: {user_prompt}\\n\"\n",
        "        f\"AI:\"\n",
        "    )\n",
        "\n",
        "    print(f\"{red}\\n> Entering conversation...\\n{reset}\")\n",
        "    if verbose:\n",
        "        print(f\"{red}Prompt after formatting:\\n{reset}\")\n",
        "        print(f\"{blue}{formatted_prompt}{reset}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{red}User Prompt:\\n{reset}\")\n",
        "        print(f\"{blue}{user_prompt}{reset}\")\n",
        "        print()\n",
        "\n",
        "    # When predict_conversation=False, stop before generating any \"Human:\" line\n",
        "    #response_text = llm.invoke(formatted_prompt, config={\"stop\": stop}).content\n",
        "    #response_text = llm.invoke(formatted_prompt, stop=stop).content\n",
        "    #response_text = llm.invoke(formatted_prompt, config={\"configurable\": {\"stop\": stop}}).content\n",
        "    if predict_conversation:\n",
        "        response_text = llm.invoke(formatted_prompt).content\n",
        "    else:\n",
        "        stop = [\"\\nHuman:\"] if not predict_conversation else None\n",
        "        response_text = llm.invoke(formatted_prompt, stop=stop).content\n",
        "\n",
        "\n",
        "    _history += f\"Human: {user_prompt}\\nAI: {response_text}\\n\"\n",
        "\n",
        "    print(f\"{red}> Finished.\\n{reset}\")\n",
        "    print(f\"{red}Response:{reset}\", f\"{green}{response_text}{reset}\")\n",
        "\n",
        "    return response_text\n",
        "\n",
        "predict_conversation = True\n",
        "verbose = False\n",
        "conversation(\"Hi, I'm learning about space. Can you tell me about planets?\",predict_conversation=predict_conversation,verbose=verbose)\n",
        "conversation(\"What's the largest planet in our solar system?\",predict_conversation=predict_conversation,verbose=verbose)\n",
        "conversation(\"How does its size compare to Earth?\",predict_conversation=predict_conversation,verbose=verbose)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Newer method: RunnableWithMessageHistory (recommended instead of ConversationChain)\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "\n",
        "# In-memory store for session histories (use a persistent store in production)\n",
        "session_store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in session_store:\n",
        "        session_store[session_id] = ChatMessageHistory()\n",
        "    return session_store[session_id]\n",
        "\n",
        "# Same system message as the default ConversationChain\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{question}\"),\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "conversation_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"session_id\": \"space_conversation\"}}\n",
        "r1 = conversation_with_history.invoke({\"question\": \"Hi, I'm learning about space. Can you tell me about planets?\"}, config=config)\n",
        "r2 = conversation_with_history.invoke({\"question\": \"What's the largest planet in our solar system?\"}, config=config)\n",
        "r3 = conversation_with_history.invoke({\"question\": \"How does its size compare to Earth?\"}, config=config)\n",
        "\n",
        "print(r1.content if hasattr(r1, \"content\") else r1)\n",
        "print(r2.content if hasattr(r2, \"content\") else r2)\n",
        "print(r3.content if hasattr(r3, \"content\") else r3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare how single-turn and multi-turn prompts handle a series of related questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single-turn prompts\n",
        "prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is its population?\",\n",
        "    \"What is the city's most famous landmark?\"\n",
        "]\n",
        "\n",
        "print(\"Single-turn responses:\")\n",
        "for prompt in prompts:\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {llm.invoke(prompt).content}\\n\")\n",
        "\n",
        "# Multi-turn prompts\n",
        "print(\"Multi-turn responses:\")\n",
        "conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
        "for prompt in prompts:\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {conversation.predict(input=prompt)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This tutorial has introduced you to the basics of single-turn and multi-turn prompt structures. We've seen how:\n",
        "\n",
        "1. Single-turn prompts are useful for quick, isolated queries.\n",
        "2. Multi-turn prompts maintain context across a conversation, allowing for more complex interactions.\n",
        "3. PromptTemplates can be used to create structured, reusable prompts.\n",
        "4. Conversation chains in LangChain help manage context in multi-turn interactions.\n",
        "\n",
        "Understanding these different prompt structures allows you to choose the most appropriate approach for various tasks and create more effective interactions with AI language models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
